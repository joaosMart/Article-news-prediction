{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-b09SEVL9viC",
        "q8aeinqe-N7J",
        "wTQQpoa-Ccsk",
        "iY2I7hzbi9hS",
        "yhF18JAclQWp",
        "tIsdGvehmPVX",
        "-POmWQOVtaWE",
        "xspkwTWuqxBC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "Ramen16 (Name not found). 2021. IMDBReview. https://github.com/Ramen16july/IMDBreview.\n",
        "\n",
        "Susan Li. 2018. Develop a NLP Model in Python & Deploy It With Flask. https://towardsdatascience.com/develop-a-nlp-model-in-python-deploy-it-with-flask-step-by-step-744f3bdd7776.\n",
        "\n",
        "Sai Durga Kamesh Kota. 2020. Deploying Flask application with ML Models on AWS EC2 Instance. https://medium.com/shapeai/deploying-flask-application-with-ml-models-on-aws-ec2-instance-3b9a1cec5e13.\n"
      ],
      "metadata": {
        "id": "FCsDcdem5dqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Installs"
      ],
      "metadata": {
        "id": "-b09SEVL9viC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# train_test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TF-idF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import scipy.sparse as sparse\n",
        "\n",
        "# LSD\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "# flask deployment\n",
        "\n",
        "import pickle\n",
        "from flask import Flask,render_template,request,send_file,send_from_directory,jsonify\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-470yUe9x-y",
        "outputId": "14734be5-214e-4864-eaf0-097d4f7ebf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing unit"
      ],
      "metadata": {
        "id": "q8aeinqe-N7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_accents(text):\n",
        "    text = unicodedata.normalize('NFD', text)\\\n",
        "           .encode('ascii', 'ignore')\\\n",
        "           .decode(\"utf-8\")\n",
        "\n",
        "    return str(text)\n",
        "\n",
        "def cleanPunc(sentence):\n",
        "    cleaned = re.sub(r'[?|!|â€ž|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    cleaned = cleaned.strip()\n",
        "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
        "    return cleaned\n",
        "\n",
        "def stemming(sentence):\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stemSentence = \"\"\n",
        "    for word in sentence.split():\n",
        "        stem = stemmer.stem(word)\n",
        "        if (len(stem) > 2): # small edit\n",
        "          stemSentence += stem\n",
        "          stemSentence += \" \"\n",
        "    stemSentence = stemSentence.strip()\n",
        "    return stemSentence\n",
        "\n",
        "\n",
        "def removeStopWords(sentence):\n",
        "    global re_stop_words\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.update(['zero','one','two',\n",
        "                      'three','four','five',\n",
        "                      'six','seven','eight',\n",
        "                      'nine','ten','may',\n",
        "                      'also','across','among',\n",
        "                      'beside','however','yet',\n",
        "                      'within','since'])\n",
        "\n",
        "    re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
        "\n",
        "    return re_stop_words.sub(\" \", sentence)\n",
        "\n",
        "def preprocessing(text):\n",
        "  # just do everything in one function\n",
        "  text = strip_accents(text)\n",
        "  text = cleanPunc(text)\n",
        "  text = removeStopWords(text)\n",
        "  text = stemming(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "7v99tPjw-Q-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read, process and one-hot encode"
      ],
      "metadata": {
        "id": "wTQQpoa-Ccsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('assm_4.csv')\n",
        "df = df.dropna(axis = 0).drop('Unnamed: 0', axis = 1)\n",
        "\n",
        "# One hot Encoding\n",
        "df['topics'] = df['topics'].str.replace('[', '')\n",
        "df['topics'] = df['topics'].str.replace(']', '')\n",
        "df['topics'] = df['topics'].str.replace(\"' \", '')\n",
        "df['topics'] = df['topics'].str.replace(\"'\", '')\n",
        "\n",
        "df_dummy = (df['topics'].str.replace(\", \", ',')   # remove all spaces\n",
        "    .str.get_dummies(',')            # get the dummies\n",
        ")\n",
        "\n",
        "df = pd.concat([df,df_dummy], axis = 1)\n",
        "\n",
        "# Remove not necessary\n",
        "df = df.drop('Archive', axis = 1)\n",
        "df_dummy = df_dummy.drop('Archive',axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg6tFHwK-6eR",
        "outputId": "1645007b-72ee-4ccb-cab1-c982da1bcb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f72159e221a3>:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df['topics'] = df['topics'].str.replace('[', '')\n",
            "<ipython-input-5-f72159e221a3>:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df['topics'] = df['topics'].str.replace(']', '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so we don't run into any trouble\n",
        "df[\"headline\"] = df[\"headline\"].astype(str)\n",
        "df[\"body\"] = df[\"body\"].astype(str)"
      ],
      "metadata": {
        "id": "SnFAN1q3fSRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"processed\"] = df[\"body\"].apply(lambda x : preprocessing(x))"
      ],
      "metadata": {
        "id": "TSBU9icmf52F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF + SVD"
      ],
      "metadata": {
        "id": "iY2I7hzbi9hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tc = TfidfVectorizer( stop_words='english',\n",
        "                      max_features= 1500, # found with experimentation\n",
        "                      max_df = 0.75,\n",
        "                      smooth_idf=True)\n",
        "X = tc.fit_transform(df[\"processed\"])"
      ],
      "metadata": {
        "id": "xW3V1FCpjiaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd_model = TruncatedSVD(n_components = 500,\n",
        "                         algorithm='randomized',\n",
        "                         n_iter=100,\n",
        "                         random_state=122)\n",
        "X = svd_model.fit_transform(X)"
      ],
      "metadata": {
        "id": "FOoDcbMZjpwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN Model"
      ],
      "metadata": {
        "id": "yhF18JAclQWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Dense(250, input_dim=X.shape[1], kernel_initializer='he_uniform',activation='relu'))\n",
        "model.add(layers.Dense(150,activation='relu'))\n",
        "model.add(layers.Dense(100,activation='relu'))\n",
        "model.add(layers.Dense(50, activation='relu'))\n",
        "model.add(layers.Dense(9,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lihwvBVhlS47",
        "outputId": "6c808897-6279-424e-b702-2567e8f6e95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 250)               125250    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 150)               37650     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               15100     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 9)                 459       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 183,509\n",
            "Trainable params: 183,509\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "tIsdGvehmPVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df_dummy.to_numpy()"
      ],
      "metadata": {
        "id": "W1Q0rw19o6Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "YdN7aE-EmRVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 16, epochs = 10, validation_data = (X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZqrbJz3pP_i",
        "outputId": "6cf97f25-bc7f-43c7-8647-73238340130b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "52/52 [==============================] - 2s 14ms/step - loss: 0.4890 - accuracy: 0.1484 - val_loss: 0.3661 - val_accuracy: 0.1359\n",
            "Epoch 2/10\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.3577 - accuracy: 0.1898 - val_loss: 0.3291 - val_accuracy: 0.2524\n",
            "Epoch 3/10\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.2673 - accuracy: 0.5000 - val_loss: 0.2395 - val_accuracy: 0.6214\n",
            "Epoch 4/10\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.1522 - accuracy: 0.7506 - val_loss: 0.2177 - val_accuracy: 0.6893\n",
            "Epoch 5/10\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0897 - accuracy: 0.8236 - val_loss: 0.2302 - val_accuracy: 0.6262\n",
            "Epoch 6/10\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0562 - accuracy: 0.8066 - val_loss: 0.2490 - val_accuracy: 0.6553\n",
            "Epoch 7/10\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0380 - accuracy: 0.7847 - val_loss: 0.2799 - val_accuracy: 0.6214\n",
            "Epoch 8/10\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0291 - accuracy: 0.7895 - val_loss: 0.2990 - val_accuracy: 0.6408\n",
            "Epoch 9/10\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0212 - accuracy: 0.7725 - val_loss: 0.3231 - val_accuracy: 0.6214\n",
            "Epoch 10/10\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0177 - accuracy: 0.8054 - val_loss: 0.3215 - val_accuracy: 0.6214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (model.predict(X_test)).round()\n",
        "\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
        "\n",
        "confusion = multilabel_confusion_matrix(y_test.astype(float).argmax(axis=1),\n",
        "                                        y_pred.astype(float).argmax(axis=1))\n",
        "print('Confusion Matrixes: \\n')\n",
        "\n",
        "for i, j in zip(df_dummy.columns, confusion):\n",
        "  print('\\n' + i + ':')\n",
        "  print(j)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05OyzW_XpW78",
        "outputId": "b235cfd3-7441-4773-921c-f0cc5dd5cc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 4ms/step\n",
            "\n",
            "Accuracy: 0.55\n",
            "\n",
            "Weighted Precision: 0.80\n",
            "Weighted Recall: 0.69\n",
            "Weighted F1-score: 0.73\n",
            "Confusion Matrixes: \n",
            "\n",
            "\n",
            "Business:\n",
            "[[148  27]\n",
            " [  3  28]]\n",
            "\n",
            "Culture:\n",
            "[[161   4]\n",
            " [ 12  29]]\n",
            "\n",
            "Nature:\n",
            "[[159   5]\n",
            " [ 17  25]]\n",
            "\n",
            "Podcast:\n",
            "[[153   6]\n",
            " [ 15  32]]\n",
            "\n",
            "Politics:\n",
            "[[161  17]\n",
            " [ 13  15]]\n",
            "\n",
            "Sci&Tech:\n",
            "[[185   4]\n",
            " [  3  14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCw3q6O0pd3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Dump"
      ],
      "metadata": {
        "id": "-POmWQOVtaWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Dumping the model object to save it as model.pkl file\n",
        "\n",
        "  pickle.dump(tc, open('tfidf.pkl', 'wb+'))\n",
        "  pickle.dump(svd_model,open('model_svd.pkl','wb+'))\n",
        "  #pickle.dump(model,open('model.pkl','wb+'))\n",
        "  model.save('model.h5')"
      ],
      "metadata": {
        "id": "mjsw5nFuqydA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "xspkwTWuqxBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# html and style templates\n",
        "ZipFile('templates.zip','r').extractall()\n",
        "ZipFile('static.zip','r').extractall()\n",
        "\n",
        "# init flask\n",
        "app = Flask(__name__)\n",
        "\n",
        "# load models\n",
        "svd_model=pickle.load(open('model_svd.pkl','rb+'))\n",
        "#model=pickle.load(open('model.pkl','rb+'))\n",
        "tfidf =pickle.load(open('tfidf.pkl','rb+'))\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# home page\n",
        "@app.route('/')\n",
        "def home():\n",
        "\treturn render_template('home.html')\n",
        "\n",
        "# predict\n",
        "@app.route('/predict',methods=['POST'])\n",
        "def predict():\n",
        "  # categories for a good clean get\n",
        "  columns = np.array(['Business', 'Culture', 'Nature', 'Podcast', 'Politics', 'Sci&Tech','Society', 'Sport', 'Travel'])\n",
        "\n",
        "  if request.method == 'POST':\n",
        "    # get user input\n",
        "    s = request.form['message']\n",
        "\n",
        "    # process and predict\n",
        "    sample = [preprocessing(s)]\n",
        "    X = tfidf.transform(sample)\n",
        "    input = svd_model.transform(X)\n",
        "    pred = model.predict(input)\n",
        "    b = pred.round().astype(bool)[0]\n",
        "\n",
        "    # get output ready\n",
        "    if sum(b) > 0:\n",
        "    \toutput = \"\"\n",
        "    \tfor i in columns[b]:\n",
        "    \t\toutput = output + \" \" + i\n",
        "    else:\n",
        "    \toutput = \"This has no category\"\n",
        "\n",
        "  return render_template('result.html', prediction = output)\n",
        "\n",
        "# run it\n",
        "if __name__ == '__main__':\n",
        "\tapp.run(host='0.0.0.0',port=8080)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfd92dEQtPlH",
        "outputId": "e94aa32d-7e12-4b29-e931-5950b3ebc611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras model archive loading:\n",
            "File Name                                             Modified             Size\n",
            "variables.h5                                   2023-02-28 18:40:40      2235164\n",
            "metadata.json                                  2023-02-28 18:40:40           64\n",
            "config.json                                    2023-02-28 18:40:40         2674\n",
            "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
            "...layers\n",
            "......dense\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_1\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_2\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_3\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_4\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...metrics\n",
            "......mean\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......mean_metric_wrapper\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...optimizer\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            ".........10\n",
            ".........11\n",
            ".........12\n",
            ".........13\n",
            ".........14\n",
            ".........15\n",
            ".........16\n",
            ".........17\n",
            ".........18\n",
            ".........19\n",
            ".........2\n",
            ".........20\n",
            ".........3\n",
            ".........4\n",
            ".........5\n",
            ".........6\n",
            ".........7\n",
            ".........8\n",
            ".........9\n",
            "...vars\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    }
  ]
}